{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHtaPysVSNZN"
   },
   "source": [
    "# Step 1 - Install the required dependencies and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "df = pd.DataFrame(ds['test']).head(500)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(x):\n",
    "    if x == 0:\n",
    "        return 'negative'\n",
    "    elif x == 1:\n",
    "        return 'neutral'\n",
    "    elif x == 2:\n",
    "        return 'positive'\n",
    "    return x\n",
    "df['label'] = df['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Run model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: This step is going to download two models of ~500MB each. \n",
    "\n",
    "**If you don't want to download the models, you can jump to step 4 and use the provided data in the repo instead.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "results = []\n",
    "texts = df['text'].to_list()\n",
    "\n",
    "## Depending on your machine, this should take around 1 minute\n",
    "for text in tqdm.tqdm(texts):\n",
    "    results.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['roberta'] = [r[0]['label'] for r in results]\n",
    "df['roberta_score'] = [r[0]['score'] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"LYTinn/finetuning-sentiment-model-tweet-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "results = []\n",
    "texts = df['text'].to_list()\n",
    "\n",
    "## Depending on your machine, this should take around 1 minute\n",
    "for text in tqdm.tqdm(texts):\n",
    "    results.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gpt2'] = [r[0]['label'] for r in results]\n",
    "df['gpt2_score'] = [r[0]['score'] for r in results]\n",
    "\n",
    "## map labels back\n",
    "def label_map(x):\n",
    "    if x == 'LABEL_0':\n",
    "        return 'negative'\n",
    "    elif x == 'LABEL_1':\n",
    "        return 'neutral'\n",
    "    elif x == 'LABEL_2':\n",
    "        return 'positive'\n",
    "    return x\n",
    "df['gpt2'] = df['gpt2'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Pre-processing data and add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you skip the model inference, uncomment the code below and load the provided data\n",
    "\n",
    "# df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input_length\"] = df[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Start Zeno for interactive slicing\n",
    "\n",
    "In this step, you need to create 5 slices in the Zeno interface and derive meaningful insights.\n",
    "\n",
    "As a starting point, try to create the two slices we provide:\n",
    "\n",
    "1. Tweets with hashtags\n",
    "2. Tweets with strong positive words (e.g., love) -- you can determine the exact words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating slices in Zeno is straightforward: Just click on the '+' button for 'create a new slice', and you can define the slice using existing column attributes, with simple value macthing or even regular expression.\n",
    "\n",
    "![image.png](images/image.png)\n",
    "\n",
    "There are more fun features in Zeno, including interactive metadata & model comparison -- feel free to check the teaser video in [README](https://github.com/zeno-ml/zeno) of the Zeno repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no id_column specified, using index as id_column. If you are using a data_column, suggest using it as id_column.\n",
      "Running predistill functions\n",
      "\u001b[1mZeno\u001b[0m running on http://localhost:8231\n",
      "\n",
      "\n",
      "Running inference\n",
      "Running postdistill functions\n",
      "Done processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/applications.py\", line 289, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/routing.py\", line 443, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/applications.py\", line 289, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 192, in run_endpoint_function\n",
      "    return await run_in_threadpool(dependant.call, **values)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/starlette/concurrency.py\", line 41, in run_in_threadpool\n",
      "    return await anyio.to_thread.run_sync(func, *args)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 61, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/server.py\", line 178, in get_metrics_for_slices\n",
      "    return zeno.get_metrics_for_slices(req.metric_keys, req.filter_ids)\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/backend.py\", line 478, in get_metrics_for_slices\n",
      "    metric = self.calculate_metric(\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/backend.py\", line 575, in calculate_metric\n",
      "    return self.metric_functions[metric](df, local_ops).metric\n",
      "  File \"/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/api.py\", line 156, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/mx/0stgg0m56051q1f81rpvwskw0000gn/T/ipykernel_86357/1105304987.py\", line 25, in accuracy\n",
      "    return MetricReturn(metric=avg)\n",
      "  File \"pydantic/main.py\", line 347, in pydantic.main.BaseModel.__init__\n",
      "pydantic.error_wrappers.ValidationError: 1 validation error for MetricReturn\n",
      "metric\n",
      "  value is not a valid float (type=type_error.float)\n",
      "/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/processing/histogram_processing.py:95: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  filt_df.groupby([pd.cut(filt_df[str(col)], bucs)])  # type: ignore\n",
      "/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/processing/histogram_processing.py:95: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  filt_df.groupby([pd.cut(filt_df[str(col)], bucs)])  # type: ignore\n",
      "/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/processing/histogram_processing.py:95: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  filt_df.groupby([pd.cut(filt_df[str(col)], bucs)])  # type: ignore\n",
      "/Users/lawrenceegharevba/mlip_labs/cmu-mlip-model-testing-lab/venv/lib/python3.10/site-packages/zeno/processing/histogram_processing.py:95: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  filt_df.groupby([pd.cut(filt_df[str(col)], bucs)])  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "## Execute the code here to start a local Zeno server\n",
    "\n",
    "#from zeno import zeno\n",
    "\n",
    "#from zeno.api import model, distill, metric\n",
    "#from zeno.api import ModelReturn, MetricReturn, DistillReturn, ZenoOptions\n",
    "\n",
    "#@model\n",
    "#def load_model(model_name):\n",
    "    \n",
    "    #def pred(df, ops: ZenoOptions):\n",
    "        #out = df[model_name]\n",
    "        #return ModelReturn(model_output=out)\n",
    "\n",
    "    #return pred\n",
    "\n",
    "#@distill\n",
    "#def label_match(df, ops: ZenoOptions):\n",
    "    #results = (df[ops.label_column] == df[ops.output_column]).to_list()\n",
    "    #return DistillReturn(distill_output=results)\n",
    "\n",
    "#@metric\n",
    "#def accuracy(df, ops: ZenoOptions):\n",
    "    #avg = df[ops.distill_columns[\"label_match\"]].mean()\n",
    "    #return MetricReturn(metric=avg)\n",
    "\n",
    "#zeno({\n",
    "    \"metadata\": df, # Pandas DataFrame with a row for each instance\n",
    "    \"view\": \"text-classification\", # The type of view for this data/task\n",
    "    \"data_column\": \"text\", \n",
    "    \"label_column\": \"label\",\n",
    "    \"functions\": [load_model, label_match, accuracy],\n",
    "    \"models\": [\"roberta\", \"gpt2\"],\n",
    "    \"port\": 8231\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, you should be able to access Zeno in http://localhost:8231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices in the Zeno interface.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. [YOUR CHOICE]\n",
    "4. [YOUR CHOICE]\n",
    "5. [YOUR CHOICE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write down descriptions of additional slices you created\n",
    "\n",
    "custom_slice_descriptions = [\n",
    "    \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Evaluation (Pre-LLM Generated Examples)\n",
    "\n",
    "| Slice Name               | Rationale                                                                 | Observation                                                                                         | Implication                                                                                   |\n",
    "|---------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Positive Label (Tweets)   | Hypothesis is that the model should perform strongly on clearly positive sentiment, since training data often has abundant positive examples. | Accuracy is high (~70%). Predictions were consistent, though occasionally tweets with sarcasm (â€œlove waiting in trafficâ€) were misclassified. | Model is strong at detecting explicit positivity, but sarcasm or subtle positivity can cause errors. |\n",
    "| Negative Tweets           | Hypothesis was that tweets with negative cues (cancel, criticism, decline) would be harder for the model. | Accuracy dropped to 0.60. Misclassifications clustered around sarcasm, political critique, and factual decline statements. | Model struggles with subtle negativity and context, defaulting to neutral or misreading factual decline as sentiment. |\n",
    "| Long Tweets               | Longer tweets may introduce ambiguity or mixed sentiment, challenging the model. | Accuracy = 0.70. Misclassifications clustered around political critique, sarcasm, and factual decline statements. Clear sentiment tweets were correctly classified. | Model struggles with nuanced language in long tweets, defaulting to neutral or misinterpreting factual tone as sentiment. |\n",
    "| Low Confidence Predictions | Hypothesis was that low confidence scores indicate ambiguous or difficult cases. | Accuracy = 0.55. Misclassifications clustered around emotional nuance, sarcasm, factual critique, and political framing. Correct predictions were mostly straightforward neutral or strongly negative tweets. | Confidence scores are a useful diagnostic â€” low confidence reliably flags examples where the model is unstable or error prone. |\n",
    "| Ambiguous Tweets          | Hypothesis was that evaluative phrasing (â€œcozied upâ€) should be classified as negative. | Model predicted neutral, showing a misclassification. | The model underestimates subtle negative sentiment in political contexts. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Write down three addition data slices you want to create but do not have the metadata for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, you might have already come up with some slices you wanted to create but found it hard to do with existing metadata. Write down three of such slices in this step.\n",
    "\n",
    "Example: \n",
    "- I want to create a slice on tweets using slangs\n",
    "- I want to create a slice on non-English tweets (if any)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Write down three additional data slices here:\n",
    "\n",
    "additional_slice_descriptions = [\n",
    "    \"I want to create a slice on tweets using emoji\n",
    "\n",
    "    want to create a slice on tweets using slang,\n",
    "\n",
    "    want to create a slice on tweets using question tweets\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - OH: â€œI had a blue penis while I was thisâ€ [playing with Google Earth VR]\n",
    "> - @user @user Thatâ€™s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write down the slice you select\n",
    "\n",
    "slice_description = \"Rationale\n",
    "\n",
    "â€¢\tEmojis often carry sentiment (ğŸ˜‚ = positive, ğŸ˜­ = negative/sad, ğŸ˜¡ = anger).\n",
    "\n",
    "â€¢\tHypothesis: The model may misclassify tweets where emojis contradict or amplify the text sentiment.\n",
    "\"\n",
    "\n",
    "## Write down all generated test cases here\n",
    "\n",
    "generated_test_cases = [\n",
    "\n",
    "1.\tâ€œBest day ever ğŸ˜‚ğŸ˜‚ğŸ˜‚â€ â†’ Positive\n",
    "\n",
    "2.\tâ€œI canâ€™t believe this happened ğŸ˜­â€ â†’ Negative\n",
    "\n",
    "3.\tâ€œAnother Mondayâ€¦ ğŸ˜’â€ â†’ Negative\n",
    "\n",
    "4.\tâ€œSo proud of my team ğŸ‰â€ â†’ Positive\n",
    "\n",
    "5.\tâ€œThat exam was brutal ğŸ˜¡â€ â†’ Negative\n",
    "\n",
    "6.\tâ€œFinally finished my project ğŸ™Œâ€ â†’ Positive\n",
    "\n",
    "7.\tâ€œI guess itâ€™s fineâ€¦ ğŸ¤·â€ â†’ Neutral\n",
    "\n",
    "8.\tâ€œLove this song â¤ï¸â€ â†’ Positive\n",
    "\n",
    "9.\tâ€œTraffic againâ€¦ ugh ğŸ˜©â€ â†’ Negative\n",
    "\n",
    "10.\tâ€œNot sure what to think ğŸ¤”â€ â†’ Neutral\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LLM-Generated test cases (dataset) to Zeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LLM-Generated dataset to Zeno\n",
    "texts = [\n",
    "    \"Best day ever ğŸ˜‚ğŸ˜‚ğŸ˜‚\",\n",
    "    \"I canâ€™t believe this happened ğŸ˜­\",\n",
    "    \"Another Mondayâ€¦ ğŸ˜’\",\n",
    "    \"So proud of my team ğŸ‰\",\n",
    "    \"That exam was brutal ğŸ˜¡\",\n",
    "    \"Finally finished my project ğŸ™Œ\",\n",
    "    \"I guess itâ€™s fineâ€¦ ğŸ¤·\",\n",
    "    \"Love this song â¤ï¸\",\n",
    "    \"Traffic againâ€¦ ugh ğŸ˜©\",\n",
    "    \"Not sure what to think ğŸ¤”\",\n",
    "]\n",
    "\n",
    "gold_labels = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"neutral\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"neutral\",\n",
    "]\n",
    "\n",
    "outputs = pipe(texts)\n",
    "pred_labels = [o[\"label\"] for o in outputs]\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": gold_labels,\n",
    "    \"prediction\": pred_labels,\n",
    "})\n",
    "new_data[\"correct\"] = (new_data[\"label\"] == new_data[\"prediction\"]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(new_data[\"text\"].tolist())\n",
    "new_data[\"prediction\"] = [o[\"label\"] for o in outputs]\n",
    "new_data[\"correct\"] = (new_data[\"label\"] == new_data[\"prediction\"]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = pipe(df_extended[\"text\"].tolist())\n",
    "df_extended[\"roberta\"] = [o[\"label\"] for o in all_outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeno({\n",
    "    \"metadata\": df_extended,\n",
    "    \"view\": \"text-classification\",\n",
    "    \"data_column\": \"text\",\n",
    "    \"label_column\": \"label\",\n",
    "    \"functions\": [load_model, label_match, accuracy],\n",
    "    \"models\": [\"roberta\"],  # or [\"roberta\", \"gpt2\"] if you also have that column\n",
    "    \"port\": 8231\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Evaluation (Post-LLM Generated Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "| Slice Name              | Rationale                                                                 | Observed Behavior                                                                                   | Implication                                                                                   |\n",
    "|--------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Positive Label Tweets    | Explicitly positive tweets, often with praise, enthusiasm, or supportive tone | Accuracy â‰ˆ 0.95 (20 instances). Model correctly classified clear positive sentiment (e.g., praise, celebration, emoji reinforcement). Occasional misclassifications occurred with sarcasm or subtle positivity. | Model is strong at detecting explicit positivity, especially when reinforced by emojis or hashtags. Weaknesses remain in subtle or sarcastic positive phrasing. |\n",
    "| Negative Tweets          | Explicitly negative tweets with strong language                          | Accuracy â‰ˆ 0.60 (20 instances). Model correctly classified clear negativity, but struggled with sarcasm and factual decline. | Model handles explicit negativity well but misclassifies subtle or implied negative sentiment. |\n",
    "| Long Tweets              | Longer tweets often contain multiple clauses, sarcasm, or mixed sentiment | Accuracy â‰ˆ 0.60 (20 instances). Misclassifications clustered around sarcasm, factual decline, missed enthusiasm, and emoji cues. | Model struggles with nuanced or multiâ€‘clause sentiment in long tweets.                        |\n",
    "| Low Confidence Predictions | Tweets flagged with low model confidence scores                         | Accuracy â‰ˆ 0.55â€“0.60 (20 instances). Misclassifications clustered around sarcasm, factual decline, missed enthusiasm, and emoji cues. | Confidence scores reliably highlight unstable predictions, useful for humanâ€‘inâ€‘theâ€‘loop review. |\n",
    "| Ambiguous Tweets         | Tweets with unclear sentiment, sarcasm, rhetorical questions, or factual tone | Accuracy â‰ˆ 0.70 (20 instances). Misclassifications clustered around sarcasm, factual decline, promotional tone, and ambiguous emoji use. | Model struggles with ambiguity, often defaulting to neutral or misreading tone.                |\n",
    "| Slang Tweets             | Tweets with slang, informal phrasing, or culturally loaded hashtags        | Accuracy â‰ˆ 0.71 (20 instances). Correct on slangâ€‘reinforced praise/criticism, but misclassified slang terms, promotional tone, and rhetorical critiques. | Model struggles with informal or culturally specific slang, often misinterpreting tone.        |\n",
    "| Emoji Tweets             | Emojis often carry sentiment signals that may reinforce or contradict text | Accuracy = 1.00 (20 instances). Model correctly classified positive, negative, and sarcastic emoji cases. | Model distinguishes emoji sentiment in context, but more diverse emoji testing is needed.      |\n",
    "| Question Tweets          | Tweets ending with or containing questions, often rhetorical or sarcastic | Accuracy â‰ˆ 0.66 (20 instances). Correct on explicit negative rhetorical questions, but misclassified neutral/factual inquiries and emojiâ€‘laden questions. | Model struggles with distinguishing genuine inquiries from rhetorical or sarcastic sentiment.  |\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "mlip-lab4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
